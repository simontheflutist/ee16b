\documentclass[oneside]{memoir}

\usepackage[protrusion,expansion,kerning,tracking,spacing]{microtype}

\usepackage[utf8]{inputenc}

%\usepackage{libertine}
\usepackage{lmodern}

\usepackage{multicol}

\usepackage{framed}

\usepackage[T1]{fontenc}

\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsfonts, amsthm, commath, physics}

\microtypecontext{spacing=nonfrench}
\nonfrenchspacing

%\linespread{1.23}
%
%\setlrmargins{*}{*}{1.618}
%\setulmargins{*}{*}{1.618}	
%\settypeblocksize{7.5in}{*}{0.618}
%\checkandfixthelayout

\counterwithout{section}{chapter}

\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Image}{Im}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
%\usepackage[margin=1in]{geometry}
\begin{document}
	\title{An algebraic solution to linear ODEs of order 2}
	\author{Simon Kuang}
	\maketitle
	These notes develop a theory for a certain class of linear ordinary differential equations of order 2:
	\begin{align*}
		\derivative[2]{}{t} f(t) + a_1\derivative{}{t} f(t) + a_0 f(t) &= 0
	\end{align*}
	
	\section{Prolegomena}
	\def\realfunctions{\ensuremath{C(\mathbb{R})}}
	\(f\) is a function \(\mathbb{R}\to \mathbb{R}\).
	We will restrict our discussion to ``nice'' functions---those with derivatives of all orders, and whose power series converge everywhere.
	Let \(\realfunctions\) denote all such functions.
	
	For \(f, g \in \realfunctions\), \(\alpha \in \mathbb{R}\), define
	\begin{align*}
		\begin{aligned}
			\left(f + \alpha g\right) &: &&\mathbb{R} \to \mathbb{R} \\
									  &\phantom{{}:{}} && t \mapsto f(t) + \alpha g(t)
		\end{aligned}
	\end{align*}
	
	\begin{lemma}
		\(\mathcal{F}(\mathbb{R}, \mathbb{R})\) can be considered a vector space of \(\mathbb{R}\) with the operations stated above.
	\end{lemma}
	\begin{proof}
		Exercise.
	\end{proof}
	
	Sometimes we will implicitly treat \(\realfunctions\) as a proper subspace \(C(\mathbb{C})\), and the vector space structure is preserved.
	
	\begin{definition}
		If \(f\in \realfunctions\), \(\derivative[1]{}{t}f\) or \(\derivative{}{t}f\) is the element of \(\realfunctions\) defined by \(t \mapsto \lim_{\delta\to 0}\frac{f(t + \delta) - f(t)}{\delta}\).
	\end{definition}
	\begin{definition}
		For a positive integer \(n\) and \(f\in\realfunctions\), \(\derivative[n]{}{t} f = \derivative{}{t} \derivative[n - 1]{}{t} f\).
	\end{definition}
	When the independent variable is unambiguous, we will denote \(\derivative{}{t}\) by \(D:\realfunctions\to\realfunctions\).
	\begin{lemma}
		\(D\) is a linear operator.
	\end{lemma}
	\begin{proof}
		Exercise.
	\end{proof}
	
	\begin{definition}
		There is an identity map on \(\realfunctions\). Call it \(1\).
	\end{definition}
	\begin{lemma}
		\(1:\realfunctions\to\realfunctions\) is linear.
	\end{lemma}
	\begin{proof}
		Exercise.
	\end{proof}
	
	\begin{lemma}
		Every polynomial of degree 2 splits into two linear factors.
	\end{lemma}
	\begin{proof}
		Follows from the quadratic formula (you don't need the Fundamental Theorem of Algebra for this).
	\end{proof}
	
	\section{Derivation}
	
	We want to look for which \(f\in\realfunctions\) satisfying the following equation:
	\begin{align}
		\derivative[2]{}{t} f(t) + a_1\derivative{}{t} f(t) + a_0 f(t) &= 0
%	\end{align}
%	\intertext{Recall that continuous functions \(f: \mathbb{R} \to \mathbb{R}\) form a vector space over \(\mathbb{R}\) under pointwise operations. Call this space \(C(\mathbb{R})\). There is a linear operator \(D: C(\mathbb{R}) \to  C(\mathbb{R})\) that sends \(f\) to \(\frac{d}{dt}f\). Let \(I\) be the identity map on \(C(\mathbb{R})\). Let \(0:C(\mathbb{R}) \to  0_\mathbb{R}\) refer to the zero vector. Use these facts to rewrite our differential equation (linear operators are like square matrices):}
		\intertext{Rewrite this equation using the linear operators on \(\realfunctions\) that we defined above.}
%	\begin{align}
		D^2 f + (a_1 D) f + (a_0 1)f &= 0
%	\end{align}
%	\intertext{Linear transformations preserve linear combinations, so `factor' the operators out of the LHS expression (this looks like factoring, and indeed it is, if you imagine matrix multiplication):}
	\intertext{Factor the operators using linearity.}
%	\begin{align}
		(D^2 + a_1D + a_01)f &= 0
%	\intertext{Notice that the \(D\)s look like a polynomial on the LHS. (For the nerds: this is the minimal polynomial of \(D\) restricted to our solution space.) 
	%	Over \(C(\mathbb{R})\), \(D\) has un
%		The Fundamental Theorem of Algebra says we can split this monic polynomial, with `roots' \(\lambda_1, \lambda_2 \in \mathbb{C}\).}
		\intertext{Factor the polynomial in \(D\).}
	(D - \lambda_11)(D - \lambda_21)f &= 0
%	\intertext{
	\end{align}
%	The two operators on the left, \(D - \lambda_1I\) and \(D - \lambda_2I\), commute, as you could multiply them out in any order and you'd get the same polynomial in \(D\). 
	The identity operator \(1\) is omitted, by abuse of notation.
	\begin{lemma}
		\((D - \lambda_1)(D - \lambda_2) = (D - \lambda_2)(D - \lambda_1)\); that is, \((D - \lambda_1)\) and \((D - \lambda_2)\) commute.
	\end{lemma}
	\begin{proof}
		Multiply them out; they're polynomials.
	\end{proof}
	
	\begin{lemma}
		If \(\lambda_1 \neq \lambda_2\) and \((D - \lambda_1)(D - \lambda_2)f = 0\), then \(f\) lies in \(\operatorname{Span}\left\{\ker (D - \lambda_1), \ker (D - \lambda_2)\right\}\).
	\end{lemma}
	\begin{proof}
		Eigenspaces corresponding to different eigenvalues are linearly independent.
%		Suppose \((D - \lambda_2) f = v \neq 0\). Then \(\left(D - \lambda_1\right)v = 0\).
%		But all eigenspaces are either fixed or annihilated by operators of the form \(D - \lambda\), so \(\left(D - \lambda_1\right)f = 0\).
		
%		To see that \(\left(D - \lambda_1\right)v = 0\) if and only if \(\left(D - \lambda_1\right)f = 0\)
%		Consider the ring \(\mathbb{C}[D]\) of polynomials in \(D\). Then let \(\eta: \mathbb{C}[D] \to C(\mathbb{C})\) be evaluation at \(f\) and \(\det: \mathbb{C}[D] \to \mathbb{C}\) be the determinant function. Both \(\eta\) and \(\det\) are ring homomorphisms. Let \(R\) be the quotient ring \(\mathbb{C}[D]/(\ker \eta)\).
	\end{proof}
	
	Now there are two possibilities:
	\begin{itemize}
		\item \(\lambda_1 \neq \lambda_2\). \(f\) can have a part of \(\lambda_1\)'s eigenspace and of \(\lambda_2\)'s eigenspace, and the corresponding eigenfunctions form a basis for our solution: \(f(t) = c_1 e^{\lambda_1t} + c_2 e^{\lambda_2t}\).
	
		\item \(\lambda_1 = \lambda_2\). This is the hairy case. If \(D\) is diagonalizable, then you get something roughly similar to the above case, with \(f' = \lambda f\) and \(f'' = \lambda f'\), and then \(f(t) = Ce^{\lambda t}\). Otherwise \(D\) has a shear component and doesn't seem to have enough eigenspaces, so a lot more complicated work is necessary to get the time-varying \(tCe^{\lambda t}\) term.
	\end{itemize}
	
	\section{Extensions}
	\subsection{Higher-order ODEs}
	A similar approach works for a higher-order ODE, \((D^n + a_{n - 1} D^{n - 1} + \ldots + a_0)f\) (the FTA is necessary), but when the roots are not all distinct, the solution for \(f\) potentially has even more exotic terms whose factors are powers of \(t\).
	
	\subsection{Non-homogeneous ODEs}
	Let \(\pi\in k[D]\) be a polynomial in \(D\), where \(k\) is a subfield of \(\mathbb{C}\). The ODE \(\pi f = k\), where \(Dk = 0\), can be written in the indeterminate \(f' = f - k/a_0\), which vanishes under some related polynomial \(\pi'\) of the same order: \(\pi'f' = 0\).
\end{document}
